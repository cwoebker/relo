#!/usr/bin/env python
#encoding: utf-8

import sys, time
import math
import argparse
from relo.core import config
from relo.local.search import Search
from relo.net import crawl as rawl
from relo import yapsy # Needed for pyinstaller compilation

__author__ = "cwoebker"
__version__ = config.get_version()
__copyright__ = "cwoebker"
__license__ = "See in LICENSE file"

def main(argv):
    """
    parses the arguments and starts the application
    """
    parser = argparse.ArgumentParser(description='Recursive Document Content Search in Python')
    parser.add_argument('-v', '--version', action='version',
                        version=('%(prog)s ' + __version__))
    reloParsers = parser.add_subparsers(help='sub-command help')

    ##### Local Argumnets #####

    search = reloParsers.add_parser('search', help='search help')
    search.set_defaults(which='search')
    search.add_argument('search_key', action='store', help='keyword to search for')
    
    search.add_argument('-s', '--hidden', '--secret', action='store_true',
                        help='search hidden files')
    search.add_argument('-l', '--links', '--symbolic', action='store_true',
                        help='search in symbolically linked directories and files')
    search.add_argument('--filelog', action='store_true',
                        help='log is written to file - always in debug mode')
    search.add_argument('-r', '--recursive', action='store_true',
                        help='search recursively')
    doctype_group = search.add_mutually_exclusive_group()
    doctype_group.add_argument('-a', '--all', action='store_true',
                        help='search all files (even non supported with standard plugin)')
    doctype_group.add_argument('--doctype', action='store',
                        help='specify doctypes you want to use in your search')
    search_type_group = search.add_mutually_exclusive_group()
    search_type_group.add_argument('-n', '--name', action='store_true',
                            help='search match in fileNames (regex allowed) - (default)')
    search_type_group.add_argument('-c', '--content', action='store_true',
                            help='search match in content (regex allowed)')
    log_group = search.add_mutually_exclusive_group()
    log_group.add_argument('--info', action='store_true',
                            help='enable info mode')
    log_group.add_argument('--debug', '--verbose', action='store_true',
                        help='enable debug/verbose mode')

    search.add_argument('-d', '--directory', action='store', default='./',
                        dest='directory', help='select Directory - (default=current)')

    ##### Remote Argumnets #####

    crawl = reloParsers.add_parser('crawl', help='crawl help')
    crawl.set_defaults(which='crawl')
    crawl.add_argument('url', action='store', help='url to use')
    
    try:
        results = parser.parse_args(args=argv)
        print results
    except IOError, msg:
        parser.error(str(msg))
        return 1
    
    if results.which == 'crawl':
        url = results.url

        sTime = time.time()

        crawler = rawl.Crawler(url, 16)
        crawler.crawl()
        print "\n".join(crawler.urls)

        eTime = time.time()
        tTime = eTime - sTime

        print "Found:    %d" % crawler.links
        print "Followed: %d" % crawler.followed
        print "Stats:    (%d/s after %0.2fs)" % (int(math.ceil(float(crawler.links) / tTime)), tTime)
    elif results.which == 'search':
        search = Search(results.info, results.debug, results.all, results.hidden, results.links, results.filelog, results.content, results.recursive,
                    results.doctype, results.directory, results.search_key)
        search.log.debug(results)
        search.list()
        search.filter()
        search.start()

if __name__ == '__main__':
    main(sys.argv[1:])